{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'my_data_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c4d652c3d64b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# My classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_data_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_decoder_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecoderGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'my_data_generator'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "#libs for helper functions\n",
    "import re\n",
    "import math\n",
    "#import multiprocessing\n",
    "import random\n",
    "# Keras model functions and classes\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Data generator class for keras Sequential model\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, seqs, vocab, vocab_size, maxlen=60, batch_size=32, shuffle=False):\n",
    "        self.seqs = seqs\n",
    "        self.vocab = vocab\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(np.floor(len(self.seqs) / self.batch_size))\n",
    "\n",
    "# Generate one batch of data\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        # get indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Get sequences\n",
    "        seqs_temp = [self.seqs[k] for k in indexes]\n",
    "\n",
    "        # Generate data for model X are input contexts and y are output layers\n",
    "        X, y = self.__data_generation(seqs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# update indexes after each epoch if shuffle is true\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.seqs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "#Generate input, output data for model training given n=batch_size sequences\n",
    "    def __data_generation(self, seqs_temp):\n",
    "        sequences = list()\n",
    "        #create all sub sequences, e.g. seq = [1,2,3], then sequences = [[1],[1,2],[1,2,3]]\n",
    "        for seq in seqs_temp:\n",
    "            for i in range(1, len(seq)):\n",
    "                sequence = seq[:i+1]\n",
    "                sequences.append(sequence)\n",
    "        # pad sequences, e.g. if maxlen = 4, then sequences = [[0,0,0,1],[0,0,1,2],[0,1,2,3]]\n",
    "        sequences = pad_sequences(sequences, maxlen=self.maxlen, padding='pre')\n",
    "        sequences = np.array(sequences)\n",
    "        # create context and output split, e.g. [[0,0,0],[0,0,1],[0,1,2]], [[1],[2],[3]]\n",
    "        X, y = sequences[:,:-1],sequences[:,-1]\n",
    "        # create one hot vector for output category layer\n",
    "        y = keras.utils.to_categorical(y, num_classes=self.vocab_size)\n",
    "\n",
    "        return X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Class which contains all necessary functions for decoders for production score calculation\n",
    "class DecoderGenerator():\n",
    "    def __init__(self, model, generator, k):\n",
    "        # A Sequential model\n",
    "        self.model = model\n",
    "        #DataGenerator\n",
    "        self.generator = generator\n",
    "        # nb of beams for beam search\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    # get all sub sequences for each sequence in train and pad\n",
    "    def prepare_seq(self, seq):\n",
    "        sequences = list()\n",
    "        #create all sub sequences, e.g. seq = [1,2,3], then sequences = [[1],[1,2],[1,2,3]]\n",
    "        for i in range(1, len(seq)):\n",
    "            sequence = seq[:i+1]\n",
    "            sequences.append(sequence)\n",
    "        # pad sequences, e.g. if maxlen = 4, then sequences = [[0,0,0,1],[0,0,1,2],[0,1,2,3]]\n",
    "        sequences = pad_sequences(sequences, maxlen=self.generator.maxlen, padding='pre')\n",
    "        sequences = np.array(sequences)\n",
    "        # create context and output split, e.g. [[0,0,0],[0,0,1],[0,1,2]], [[1],[2],[3]]\n",
    "        X, y = sequences[:,:-1],sequences[:,-1]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    # Returns the log probability of a sequence of words given the current context and the next possible word.\n",
    "    def get_seq_prob(self, word, context):\n",
    "        # create copy of context and add next word\n",
    "        sub_seq = copy.deepcopy(context)\n",
    "        sub_seq.append(word)\n",
    "        # prepare sequence, such that x represents the contexts for each word y in the sequence, e.g. for sequence [1,2,3], x,y = [[],[1],[1,2]], [[1],[2],[3]]\n",
    "        x, y = self.prepare_seq(sub_seq)\n",
    "        # get output layers for each state in x\n",
    "        p_pred = self.model.predict(x)\n",
    "        # accumulate probability at each state to return probability of whole sequence\n",
    "        log_p_seq = 0.0\n",
    "        for i, prob in enumerate(p_pred):\n",
    "            prob_word = prob[y[i]]\n",
    "            log_p_seq += np.log(prob_word)\n",
    "\n",
    "        return log_p_seq\n",
    "\n",
    "    # Performs beam search decoder estimation for sequence and then returns 1 if original sequence is in final k beams else returns 0 .\n",
    "    def beam_search_decoder(self, seq):\n",
    "        result = 0\n",
    "        # create \"bag of words\" from original sequence\n",
    "        vocab = list(seq)\n",
    "        # beams are composed of a context, the remaining vocab, and a score\n",
    "        beams = [[list(), vocab, np.log(1.0)]]\n",
    "\n",
    "        for i in range(len(seq)):\n",
    "            # keep track of all possible candidates for beams at each state\n",
    "            candidates = []\n",
    "            for (context, vocab, score) in beams:\n",
    "                # for each beam, find all possible next states and their scores\n",
    "                for v in range(len(vocab)):\n",
    "                    score = self.get_seq_prob(vocab[v], context)\n",
    "                    # remove item from vocab and add it to the context for the new candidate beam\n",
    "                    new_vocab = vocab[:v] + vocab[(v + 1):]\n",
    "                    new_context = copy.deepcopy(context)\n",
    "                    new_context.append(vocab[v])\n",
    "                    candidates.append([new_context, new_vocab, score])\n",
    "            # order all candidate beams next state according to their scores\n",
    "            ordered = sorted(candidates, key=lambda prob: prob[2], reverse=True)\n",
    "            # keep top k beams for the next iteration\n",
    "            if self.k < len(ordered):\n",
    "                beams = ordered[:self.k]\n",
    "            else:\n",
    "                beams = ordered\n",
    "        for context,vocab,score in beams:\n",
    "            if context == seq:\n",
    "                result = 1\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    # Performs the greedy decoder estimation and then returns 1 if it is equal to the original sequence else returns 0 .\n",
    "    def greedy_decoder(self,seq):\n",
    "        result = 0\n",
    "        # create \"bag of words\" from sequence\n",
    "        vocab = list(seq)\n",
    "        context = []\n",
    "        # while there are still words in the bag of words\n",
    "        while vocab:\n",
    "            # find the most probable next word add it to the current context and remove it from the bag of words\n",
    "            (next_word, max_prob) = max([(v, self.get_seq_prob(v, context)) for v in vocab],\n",
    "                                        key=lambda prob: prob[1])\n",
    "            context.append(next_word)\n",
    "            vocab.remove(next_word)\n",
    "        # if the greedy sequence is the same as the original return 1 else 0\n",
    "        if context == seq:\n",
    "            result = 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Returns the number of correct predictions and the overall number of test utterances for each sequence length for a given model decoder is either 'greedy' or 'beam'\n",
    "    def get_performance_bylength(self, decoder):\n",
    "\n",
    "        # Returns all sequences with less than 17 words organized by sequence length\n",
    "        def get_seq_bylength(seqs):\n",
    "            seqs_bylength = dict()\n",
    "            for seq in seqs:\n",
    "                seqlen = len(seq)\n",
    "                if 1 < seqlen < 17:\n",
    "                    if seqlen in seqs_bylength:\n",
    "                        seqs_bylength[seqlen].append(seq)\n",
    "                    else:\n",
    "                        seqs_bylength[seqlen] = [seq]\n",
    "            return seqs_bylength\n",
    "        # organize sequences by length\n",
    "        seqs_bylength = get_seq_bylength(self.generator.seqs)\n",
    "        results_bylength = dict()\n",
    "        for length, seqs in seqs_bylength.items():\n",
    "            # for each length get the nb of correct predictions and the total nb of test utterances\n",
    "            results_bylength[length] = [0, len(seqs)]\n",
    "            print(str(length))\n",
    "            for seq in seqs:\n",
    "                # use greedy decoder\n",
    "                if (decoder == 'greedy'):\n",
    "                    results_bylength[length][0] += self.greedy_decoder(seq)\n",
    "                # use beam search decoder\n",
    "                else:\n",
    "                    results_bylength[length][0] += self.beam_search_decoder(seq)\n",
    "\n",
    "        return results_bylength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HYPERPARAMETERS ####\n",
    "\n",
    "#cpus = multiprocessing.cpu_count()\n",
    "# Nb of epochs (iterations through whole train set)\n",
    "epochs=15\n",
    "# Mini-batch size necessary for initializing data generators\n",
    "batch_size = 32\n",
    "# Size of word vectors\n",
    "output_size = 100\n",
    "# Nb of hidden neurons in 1 layer of LSTM\n",
    "hidden_size = 50\n",
    "# Generate sentences in order of transcript\n",
    "shuffle = False\n",
    "# Nb of beams for beam beam_search\n",
    "k = 5\n",
    "\n",
    "\n",
    "#### GLOBAL VARIABLES ####\n",
    "\n",
    "transcript_dir = \".\"\n",
    "model_dir = \"./result\"\n",
    "result_dir = \"./result\"\n",
    "train_all_data = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS ####\n",
    "\n",
    "# A biased (p) coin flip to determine whether a child utterance will be part of train or test set\n",
    "def is_test_sent(p):\n",
    "    return True if random.random() < p else False\n",
    "\n",
    "\n",
    "# Retrieve train and test sets for all child transcripts\n",
    "def get_data_from_files():\n",
    "    data = []\n",
    "    for subdir, dirs, files in os.walk(transcript_dir):\n",
    "        for file in files:\n",
    "            if ('.capp' in file):\n",
    "                textfile = subdir+'/'+file\n",
    "                with open(textfile,'r') as f :\n",
    "                    lines = f.readlines()\n",
    "                train = []\n",
    "                test = []\n",
    "                for sent in lines :\n",
    "                    if '*CHI:' in sent :\n",
    "                        sent = re.sub('\\*[A-Z]+: ', '', sent)\n",
    "                        # if training on random 60% of child utterances and testing on 40% remaining\n",
    "                        if train_all_data:\n",
    "                            if is_test_sent(0.4):\n",
    "                                test.append(sent)\n",
    "                            else:\n",
    "                                train.append(sent)\n",
    "                        # else train only on child-directed and test on all child utterances\n",
    "                        else:\n",
    "                            test.append(sent)\n",
    "                    else :\n",
    "                        sent = re.sub('\\*[A-Z]+: ', '', sent)\n",
    "                        train.append(sent)\n",
    "                data.append((file,train,test))\n",
    "                # save test and train split in case we need to rerun model\n",
    "                with open(model_dir+'/train/'+file.split('.capp')[0]+'.train.txt','w') as f :\n",
    "                    for line in train:\n",
    "                        f.write(line)\n",
    "                with open(model_dir+'/test/'+file.split('.capp')[0]+'.test.txt','w') as f :\n",
    "                    for line in test:\n",
    "                        f.write(line)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARE DATA FOR: Will.capp\n",
      "\n",
      "vocab_size = 338\n",
      "train_maxlen = 17\n",
      "INITIALIZE DATA GENERATORS...\n",
      "\n",
      "TRAINING MODEL...\n",
      "\n",
      "Epoch 1/15\n",
      " - 1s - loss: 5.7645\n",
      "Epoch 2/15\n",
      " - 0s - loss: 5.2988\n",
      "Epoch 3/15\n",
      " - 0s - loss: 5.0949\n",
      "Epoch 4/15\n",
      " - 0s - loss: 5.0436\n",
      "Epoch 5/15\n",
      " - 0s - loss: 4.9877\n",
      "Epoch 6/15\n",
      " - 0s - loss: 4.9793\n",
      "Epoch 7/15\n",
      " - 0s - loss: 4.9725\n",
      "Epoch 8/15\n",
      " - 0s - loss: 4.9444\n",
      "Epoch 9/15\n",
      " - 0s - loss: 4.9414\n",
      "Epoch 10/15\n",
      " - 0s - loss: 4.9187\n",
      "Epoch 11/15\n",
      " - 0s - loss: 4.8875\n",
      "Epoch 12/15\n",
      " - 0s - loss: 4.8472\n",
      "Epoch 13/15\n",
      " - 0s - loss: 4.8334\n",
      "Epoch 14/15\n",
      " - 0s - loss: 4.8082\n",
      "Epoch 15/15\n",
      " - 0s - loss: 4.7658\n",
      "CALCULATING PRODUCTION PERFORMANCE METRIC 1...\n",
      "\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "CALCULATING PRODUCTION PERFORMANCE METRIC 2...\n",
      "\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "PREPARE DATA FOR: Tow.capp\n",
      "\n",
      "vocab_size = 2043\n",
      "train_maxlen = 28\n",
      "INITIALIZE DATA GENERATORS...\n",
      "\n",
      "TRAINING MODEL...\n",
      "\n",
      "Epoch 1/15\n",
      " - 12s - loss: 5.8285\n",
      "Epoch 2/15\n",
      " - 11s - loss: 5.4980\n",
      "Epoch 3/15\n",
      " - 11s - loss: 5.2412\n",
      "Epoch 4/15\n",
      " - 11s - loss: 5.0454\n",
      "Epoch 5/15\n",
      " - 11s - loss: 4.8903\n",
      "Epoch 6/15\n",
      " - 11s - loss: 4.7608\n",
      "Epoch 7/15\n",
      " - 11s - loss: 4.6440\n",
      "Epoch 8/15\n",
      " - 11s - loss: 4.5572\n",
      "Epoch 9/15\n",
      " - 11s - loss: 4.4723\n",
      "Epoch 10/15\n",
      " - 11s - loss: 4.4051\n",
      "Epoch 11/15\n",
      " - 11s - loss: 4.3564\n",
      "Epoch 12/15\n",
      " - 11s - loss: 4.3193\n",
      "Epoch 13/15\n",
      " - 11s - loss: 4.2733\n",
      "Epoch 14/15\n",
      " - 12s - loss: 4.2348\n",
      "Epoch 15/15\n",
      " - 11s - loss: 4.2056\n",
      "CALCULATING PRODUCTION PERFORMANCE METRIC 1...\n",
      "\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "8\n",
      "7\n",
      "9\n",
      "10\n",
      "CALCULATING PRODUCTION PERFORMANCE METRIC 2...\n",
      "\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "8\n",
      "7\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "data = get_data_from_files()\n",
    "\n",
    "for file,train,test in data:\n",
    "    print('PREPARE DATA FOR: '+file+'\\n')\n",
    "    # Get vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train + test)\n",
    "    vocab = tokenizer.word_index\n",
    "    # vocabulary size is equal to the vocab size + the <PAD> character used for\n",
    "    # padding sequences during training\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # transform text strings into sequences of int (representing the word's\n",
    "    # index in vocab)\n",
    "    train_seqs = tokenizer.texts_to_sequences(train)\n",
    "    test_seqs = tokenizer.texts_to_sequences(test)\n",
    "    # get the maximum length of sequences - this is needed for data generator\n",
    "    maxlen = max([len(seq) for seq in train_seqs])\n",
    "    # number of optimization iterations to see whole corpus (epoch)\n",
    "    steps_per_epoch = math.ceil(len(train_seqs)/ batch_size)\n",
    "\n",
    "    print('vocab_size = '+str(vocab_size))\n",
    "    print('train_maxlen = '+str(maxlen))\n",
    "    print('INITIALIZE DATA GENERATORS...\\n')\n",
    "\n",
    "    # Create data generators for train and test sequences\n",
    "    train_generator = DataGenerator(seqs = train_seqs,\n",
    "                                       vocab = vocab,\n",
    "                                       vocab_size = vocab_size,\n",
    "                                       maxlen = maxlen,\n",
    "                                       batch_size = batch_size,\n",
    "                                       shuffle = shuffle)\n",
    "    test_generator = DataGenerator(seqs = test_seqs,\n",
    "                                       vocab = vocab,\n",
    "                                       vocab_size = vocab_size,\n",
    "                                       maxlen = maxlen,\n",
    "                                       batch_size = batch_size,\n",
    "                                       shuffle = shuffle)\n",
    "\n",
    "    print('TRAINING MODEL...\\n')\n",
    "    # initialize model\n",
    "    model = Sequential()\n",
    "    # add initial embedding layer\n",
    "    model.add(Embedding(input_dim = vocab_size,  # vocabulary size\n",
    "                        output_dim = output_size,  # size of embeddings\n",
    "                        input_length = maxlen-1))  # length of the padded sequences minus the last output word\n",
    "    #add LSTM layer\n",
    "    model.add(LSTM(hidden_size))\n",
    "    # add layer regular densely connected layer to reshape to output size and use softmax activation for output layer\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    # use RMSprop for optimization (could also use Adam or Adagrad) and cross entropy for loss function\n",
    "    model.compile('rmsprop', 'categorical_crossentropy')\n",
    "\n",
    "    # Train LSTM\n",
    "    model.fit_generator(train_generator,\n",
    "                        steps_per_epoch = steps_per_epoch,\n",
    "                        epochs = epochs,\n",
    "                        verbose=2,\n",
    "                        max_queue_size=10,\n",
    "                        shuffle=False)\n",
    "\n",
    "    # Save trained model for future use\n",
    "    model.save(str(model_dir+'/'+file.split('.capp')[0]+'_model.h5'))\n",
    "    # Initialize decoders and get production scores by utterance length using both the greedy and the beam search decoders\n",
    "    decoders = DecoderGenerator(model,test_generator,k)\n",
    "    print('CALCULATING PRODUCTION PERFORMANCE METRIC 1...\\n')\n",
    "    results_greedy = decoders.get_performance_bylength(\"greedy\")\n",
    "    print('CALCULATING PRODUCTION PERFORMANCE METRIC 2...\\n')\n",
    "    results_beam = decoders.get_performance_bylength(\"beam\")\n",
    "\n",
    "    # save all performance results\n",
    "    with open(result_dir+'/greedy/'+file.split('.capp')[0]+'.prod_result.csv','w') as f :\n",
    "        f.write(\"iter,utterance_length,nb_utterances,produced,production_score\"+'\\n')\n",
    "        for length in results_greedy:\n",
    "            f.write('1,'+str(length)+','+\n",
    "                            str(results_greedy[length][1])+','+\n",
    "                            str(results_greedy[length][0])+','+\n",
    "                            str(results_greedy[length][0]/results_greedy[length][1])+'\\n')\n",
    "        with open(result_dir+'/beam/'+file.split('.capp')[0]+'.prod_result.csv','w') as f :\n",
    "            f.write(\"iter,utterance_length,nb_utterances,produced,production_score\"+'\\n')\n",
    "            for length in results_beam:\n",
    "                f.write('1,'+str(length)+','+\n",
    "                                str(results_beam[length][1])+','+\n",
    "                                str(results_beam[length][0])+','+\n",
    "                                str(results_beam[length][0]/results_beam[length][1])+'\\n')\n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HYPERPARAMETERS ####\n",
    "\n",
    "#cpus = multiprocessing.cpu_count()\n",
    "# Nb of epochs (iterations through whole train set)\n",
    "epochs=500\n",
    "# Mini-batch size necessary for initializing data generators\n",
    "batch_size = 6\n",
    "# Size of word vectors\n",
    "output_size = 30\n",
    "# Nb of hidden neurons in 1 layer of LSTM\n",
    "hidden_size = 10\n",
    "# Generate sentences in order of transcript\n",
    "shuffle = False\n",
    "# Nb of beams for beam beam_search\n",
    "k = 5\n",
    "\n",
    "\n",
    "#### GLOBAL VARIABLES ####\n",
    "\n",
    "transcript_dir = \".\"\n",
    "model_dir = \"./result\"\n",
    "result_dir = \"./result\"\n",
    "train_all_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data_dir):\n",
    "    data = []\n",
    "    for subdir, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if ('.h5' in file):\n",
    "                model_file = subdir + '/' + file\n",
    "                childname = file.split('_model.h5')[0]\n",
    "                trainfile = subdir + '/train/' + childname + '.train.txt'\n",
    "                testfile = subdir + '/test/' + childname + '.test.txt'\n",
    "                with open(trainfile, 'r') as f:\n",
    "                    train = f.readlines()\n",
    "                with open(testfile, 'r') as f:\n",
    "                    test = f.readlines()\n",
    "                data.append((childname, train, test))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARE DATA FOR: Tow\n",
      "\n",
      "vocab_size = 2043\n",
      "train_maxlen = 28\n",
      "INITIALIZE DATA GENERATORS...\n",
      "\n",
      "TRAINING MODEL...\n",
      "\n",
      "Epoch 1/500\n",
      " - 51s - loss: 5.8625\n",
      "Epoch 2/500\n",
      " - 52s - loss: 5.5784\n",
      "Epoch 3/500\n",
      " - 51s - loss: 5.4309\n",
      "Epoch 4/500\n",
      " - 50s - loss: 5.3249\n",
      "Epoch 5/500\n",
      " - 51s - loss: 5.2957\n",
      "Epoch 6/500\n",
      " - 51s - loss: 5.2175\n",
      "Epoch 7/500\n",
      " - 52s - loss: 5.2948\n",
      "Epoch 8/500\n",
      " - 51s - loss: 5.5232\n",
      "Epoch 9/500\n",
      " - 51s - loss: 5.7780\n",
      "Epoch 10/500\n",
      " - 51s - loss: 6.0277\n",
      "Epoch 11/500\n",
      " - 51s - loss: 6.0877\n",
      "Epoch 12/500\n",
      " - 52s - loss: 6.1218\n",
      "Epoch 13/500\n",
      " - 50s - loss: 6.1395\n",
      "Epoch 14/500\n",
      " - 50s - loss: 6.1492\n",
      "Epoch 15/500\n",
      " - 50s - loss: 6.1542\n",
      "Epoch 16/500\n",
      " - 49s - loss: 6.1549\n",
      "Epoch 17/500\n",
      " - 49s - loss: 6.1508\n",
      "Epoch 18/500\n",
      " - 50s - loss: 6.1501\n",
      "Epoch 19/500\n",
      " - 50s - loss: 6.1455\n",
      "Epoch 20/500\n",
      " - 50s - loss: 6.1449\n",
      "Epoch 21/500\n",
      " - 50s - loss: 6.1396\n",
      "Epoch 22/500\n",
      " - 50s - loss: 6.1337\n",
      "Epoch 23/500\n",
      " - 50s - loss: 6.1310\n",
      "Epoch 24/500\n",
      " - 50s - loss: 6.1237\n",
      "Epoch 25/500\n",
      " - 50s - loss: 6.1205\n",
      "Epoch 26/500\n",
      " - 50s - loss: 6.1181\n",
      "Epoch 27/500\n",
      " - 50s - loss: 6.1165\n",
      "Epoch 28/500\n",
      " - 50s - loss: 6.1150\n",
      "Epoch 29/500\n",
      " - 50s - loss: 6.1151\n",
      "Epoch 30/500\n",
      " - 50s - loss: 6.1157\n",
      "Epoch 31/500\n",
      " - 50s - loss: 6.1152\n",
      "Epoch 32/500\n",
      " - 51s - loss: 6.1169\n",
      "Epoch 33/500\n",
      " - 50s - loss: 6.1163\n",
      "Epoch 34/500\n",
      " - 50s - loss: 6.1168\n",
      "Epoch 35/500\n",
      " - 50s - loss: 6.1189\n",
      "Epoch 36/500\n",
      " - 50s - loss: 6.1191\n",
      "Epoch 37/500\n",
      " - 51s - loss: 6.1199\n",
      "Epoch 38/500\n",
      " - 51s - loss: 6.1220\n",
      "Epoch 39/500\n",
      " - 50s - loss: 6.1252\n",
      "Epoch 40/500\n",
      " - 51s - loss: 6.1239\n",
      "Epoch 41/500\n",
      " - 50s - loss: 6.1247\n",
      "Epoch 42/500\n",
      " - 50s - loss: 6.1246\n",
      "Epoch 43/500\n",
      " - 51s - loss: 6.1278\n",
      "Epoch 44/500\n",
      " - 51s - loss: 6.1290\n",
      "Epoch 45/500\n",
      " - 51s - loss: 6.1293\n",
      "Epoch 46/500\n",
      " - 49s - loss: 6.1283\n",
      "Epoch 47/500\n",
      " - 51s - loss: 6.1292\n",
      "Epoch 48/500\n",
      " - 50s - loss: 6.1305\n",
      "Epoch 49/500\n",
      " - 51s - loss: 6.1302\n",
      "Epoch 50/500\n",
      " - 50s - loss: 6.1317\n",
      "Epoch 51/500\n",
      " - 50s - loss: 6.1328\n",
      "Epoch 52/500\n",
      " - 51s - loss: 6.1339\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6f07ec2b6f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                         shuffle=False)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Save trained model for future use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = get_train_test(\"./result\")\n",
    "\n",
    "for childname,train,test in data:\n",
    "    print('PREPARE DATA FOR: '+childname+'\\n')\n",
    "    # Get vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train + test)\n",
    "    vocab = tokenizer.word_index\n",
    "    # vocabulary size is equal to the vocab size + the <PAD> character used for\n",
    "    # padding sequences during training\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # transform text strings into sequences of int (representing the word's\n",
    "    # index in vocab)\n",
    "    train_seqs = tokenizer.texts_to_sequences(train)\n",
    "    test_seqs = tokenizer.texts_to_sequences(test)\n",
    "    # get the maximum length of sequences - this is needed for data generator\n",
    "    maxlen = max([len(seq) for seq in train_seqs])\n",
    "    # number of optimization iterations to see whole corpus (epoch)\n",
    "    steps_per_epoch = math.ceil(len(train_seqs)/ batch_size)\n",
    "\n",
    "    print('vocab_size = '+str(vocab_size))\n",
    "    print('train_maxlen = '+str(maxlen))\n",
    "    print('INITIALIZE DATA GENERATORS...\\n')\n",
    "\n",
    "    # Create data generators for train and test sequences\n",
    "    train_generator = DataGenerator(seqs = train_seqs,\n",
    "                                       vocab = vocab,\n",
    "                                       vocab_size = vocab_size,\n",
    "                                       maxlen = maxlen,\n",
    "                                       batch_size = batch_size,\n",
    "                                       shuffle = shuffle)\n",
    "    test_generator = DataGenerator(seqs = test_seqs,\n",
    "                                       vocab = vocab,\n",
    "                                       vocab_size = vocab_size,\n",
    "                                       maxlen = maxlen,\n",
    "                                       batch_size = batch_size,\n",
    "                                       shuffle = shuffle)\n",
    "\n",
    "    print('TRAINING MODEL...\\n')\n",
    "    # initialize model\n",
    "    model = Sequential()\n",
    "    # add initial embedding layer\n",
    "    model.add(Embedding(input_dim = vocab_size,  # vocabulary size\n",
    "                        output_dim = output_size,  # size of embeddings\n",
    "                        input_length = maxlen-1))  # length of the padded sequences minus the last output word\n",
    "    #add LSTM layer \n",
    "    model.add(LSTM(hidden_size))\n",
    "    # add layer regular densely connected layer to reshape to output size and use softmax activation for output layer\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    # use RMSprop for optimization (could also use Adam or Adagrad) and cross entropy for loss function\n",
    "    model.compile('rmsprop', 'categorical_crossentropy')\n",
    "\n",
    "    # Train LSTM\n",
    "    model.fit_generator(train_generator,\n",
    "                        steps_per_epoch = steps_per_epoch,\n",
    "                        epochs = epochs,\n",
    "                        verbose=2,\n",
    "                        max_queue_size=10,\n",
    "                        shuffle=False)\n",
    "\n",
    "    # Save trained model for future use\n",
    "    model.save(str(model_dir+'/'+childname+'_model.h5'))\n",
    "    # Initialize decoders and get production scores by utterance length using both the greedy and the beam search decoders\n",
    "    decoders = DecoderGenerator(model,test_generator,k)\n",
    "    print('CALCULATING PRODUCTION PERFORMANCE METRIC 1...\\n')\n",
    "    results_greedy = decoders.get_performance_bylength(\"greedy\")\n",
    "    print('CALCULATING PRODUCTION PERFORMANCE METRIC 2...\\n')\n",
    "    results_beam = decoders.get_performance_bylength(\"beam\")\n",
    "\n",
    "    # save all performance results\n",
    "    with open(result_dir+'/greedy/'+childname+'.prod_result_10_30.csv','w') as f :\n",
    "        f.write(\"iter,utterance_length,nb_utterances,produced,production_score\"+'\\n')\n",
    "        for length in results_greedy:\n",
    "            f.write('1,'+str(length)+','+\n",
    "                            str(results_greedy[length][1])+','+\n",
    "                            str(results_greedy[length][0])+','+\n",
    "                            str(results_greedy[length][0]/results_greedy[length][1])+'\\n')\n",
    "        with open(result_dir+'/beam/'+childname+'.prod_result_10_30.csv','w') as f :\n",
    "            f.write(\"iter,utterance_length,nb_utterances,produced,production_score\"+'\\n')\n",
    "            for length in results_beam:\n",
    "                f.write('1,'+str(length)+','+\n",
    "                                str(results_beam[length][1])+','+\n",
    "                                str(results_beam[length][0])+','+\n",
    "                                str(results_beam[length][0]/results_beam[length][1])+'\\n')\n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
