---
title: "predict_AoA_megachild"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tools)
#library(Hmisc)
library(glue)
library(broom)
#library(broom.mixed)
#library(langcog)
library(stringr)
library(tidyverse)
library(lme4)
library(modelr)
library(purrr)


theme_set(theme_classic())
```

### Get AoA estimates
The following is based on the appendix D from the Wordbank book (Frank et al. 2019).
taken from: https://github.com/langcog/wordbank-book/blob/master/104-appendix-aoa.Rmd 
I use there Bayesian GLM with hand-tuned prior parameters model to get fitted AoA estimates on the English Words & Sentences data from Wordbank. (I take their data and model as is.)
```{r aoa_estimates}
load("../../data/aoa_predictors/eng_ws_raw_data.Rds")

ms <- eng_ws %>%
  group_by(definition, age, category) %>%
  summarise(prop = mean(value == "produces", na.rm = TRUE), 
            num_true = sum(value == "produces", na.rm = TRUE), 
            num_false = sum(value != "produces", na.rm = TRUE), 
            n = sum(c(num_true, num_false))) %>%
  filter(!is.na(category))

fit_bglm <- function(data) {
  model <- arm::bayesglm(cbind(num_true, num_false) ~ age, 
                         family = "binomial", 
                         prior.mean = .3,
                         prior.scale = c(.01),
                         prior.mean.for.intercept = 0,
                         prior.scale.for.intercept = 2.5,
                         prior.df = 1,
                         data = data)
  aoa <- -model$coefficients[["(Intercept)"]] / model$coefficients[["age"]]
  
  tibble(definition = data$definition[1],
         category = data$category[1],
         bglm_slope = model$coefficients[["age"]],
         bglm_aoa = aoa)
}

bglm_aoas <- ms %>%
  split(.$definition) %>%
  map(fit_bglm) %>%
  bind_rows

```

### Get predictors from Braginsky et al. (2019) 

```{r get_predictors}
## Data of avg_freq
load("../../data/aoa_predictors/avg_freq.RData")

## Data from Braginsky et al. (2019)
load("../../data/aoa_predictors/model_data.RData")
load("../../data/aoa_predictors/uni_joined.RData")
uni_joined_eng <- uni_joined %>% filter(language == "English (American)")

```

### Get surprisal results from Megachild LSTM

```{r get_surprisal}

file = "../../data/results/lstm-mega_child/Mega_child.aoa_result.csv"
df = read.csv(file)
#Extract child name from file name and add variable.
file_name = strsplit(file_path_sans_ext(file), "/")[[1]]
file_name = file_name[(length(file_name))]
child_name = strsplit(file_name,".aoa_result", fixed=TRUE)[[1]]
model_surprisals = df %>%
    mutate(child_name = child_name)

```

### Get frequency counts CHILDES child-directed speech data

```{r get_freq}
file = "../../data/results/lstm-mega_child/Mega_child.aoa_freq.csv"
df = read.csv(file)
#Extract child name from file name and add variable.
file_name = strsplit(file_path_sans_ext(file), "/")[[1]]
file_name = file_name[(length(file_name))]
child_name = strsplit(file_name,".aoa_freq", fixed=TRUE)[[1]]
model_freq = df %>%
    mutate(child_name = child_name)

```

### Combine all predictors into one dataframe. Impute data (i.e. create estimates for missing values) and scale all predictors. 

```{r combine_pred}

surp_model_data <- model_surprisals %>% left_join(model_freq) %>% 
  mutate(avg_freq = frequency_count) %>% 
  select(uni_lemma, avg_surprisal, avg_freq, child_name) %>% 
  mutate(uni_lemma = as.character(uni_lemma)) %>% 
  left_join(ungroup(model_data)) %>% 
  group_by(child_name) %>%
  # mutate_at(vars(!!predictors), funs(as.numeric(scale(.)))) %>%
  nest()

predictors <- c("avg_surprisal", "avg_freq", "frequency", "MLU", "final_frequency", "solo_frequency", "num_phons", "concreteness", "valence", "arousal", "babiness")

pred_sources <- list(
  c("avg_surprisal", "avg_freq"),
  c("frequency", "MLU", "final_frequency", "solo_frequency"),
  c("valence", "arousal"),
  "concreteness", "babiness", "num_phons"
)

fit_predictor <- function(pred, d) {
  xs <- pred_sources %>% discard(~pred %in% .x) %>% unlist()
  x_str <- xs %>% paste(collapse = " + ")
  lm(as.formula(glue("{pred} ~ {x_str}")), data = d) %>%
    augment(newdata = d) %>%
    select(uni_lemma, lexical_category, .fitted)
}

max_steps <- 20
iterate_predictors <- function(lang_data) {
  missing <- lang_data %>%
    gather(predictor, value, !!predictors) %>%
    mutate(missing = is.na(value)) %>%
    select(-value) %>%
    spread(predictor, missing)
  predictor_order <- lang_data %>%
    gather(predictor, value, !!predictors) %>%
    group_by(predictor) %>%
    summarise(num_na = sum(is.na(value))) %>%
    filter(num_na != 0) %>%
    arrange(num_na) %>%
    pull(predictor)
  imputation_data <- lang_data %>%
    mutate_at(vars(!!predictors),
              funs(as.numeric(Hmisc::impute(., fun = "random"))))
  for (i in 0:max_steps) {
    pred <- predictor_order[(i %% length(predictor_order)) + 1]
    imputation_fits <- fit_predictor(pred, imputation_data)
    imputation_data <- missing %>%
      select(uni_lemma, lexical_category, !!pred) %>%
      rename(missing = !!pred) %>%
      right_join(imputation_data) %>%
      left_join(imputation_fits) %>%
      mutate_at(vars(pred), funs(if_else(is.na(missing), .fitted, .))) %>%
      select(-.fitted, -missing)
  }
  return(imputation_data)
}

model_data_imputed <- surp_model_data %>%
  mutate(imputed = map(data, iterate_predictors)) 

all_predictors_data <- model_data_imputed %>%
  select(-data) %>%
  unnest() %>%
  group_by(child_name) %>%
  mutate_at(vars(predictors), funs(as.numeric(scale(.))))

save(model_data_imputed, file = "../../data/aoa_predictors/model_data_imputed_mega_child.RData")
save(all_predictors_data, file = "../../data/aoa_predictors/all_predictors_data_mega_child.RData")

```

Join AoA estimates (dependent variable) to predictors together

```{r join_data}

aoa_estimates <- bglm_aoas %>% 
  mutate(words = tolower(definition),
         aoa = bglm_aoa) %>% 
  select(words, aoa)

#aoa_estimates <- empirical_aoas %>% 
#  mutate(words = tolower(definition),
#         aoa = empirical_aoa) %>% 
#  select(words, aoa)

data <- all_predictors_data %>% 
  left_join(aoa_estimates) %>% 
  filter(!is.na(aoa)) %>% 
  filter(lexical_category != "other") %>% ungroup()
  
  #mutate(lexical_category = lexical_category %>% fct_relevel("other")) %>% ungroup()


```

All the models to fit 

```{r formulae}
#full_surp = ~ lexical_category * avg_surprisal + lexical_category * avg_freq + lexical_category * MLU + lexical_category * final_frequency + lexical_category * solo_frequency + lexical_category * num_phons + lexical_category * concreteness + lexical_category * valence + lexical_category * arousal + lexical_category * babiness

full_surp = ~ lexical_category * avg_surprisal + lexical_category * avg_freq + lexical_category * num_phons + lexical_category * concreteness + lexical_category * valence + lexical_category * arousal + lexical_category * babiness

freq_surp = ~ lexical_category * avg_freq + lexical_category * avg_surprisal

#full_set = ~ lexical_category * avg_freq + lexical_category * MLU + lexical_category * final_frequency + lexical_category * solo_frequency + lexical_category *  num_phons + lexical_category * concreteness + lexical_category * valence + lexical_category * arousal + lexical_category * babiness

full_set = ~ lexical_category * avg_freq + lexical_category *  num_phons + lexical_category * concreteness + lexical_category * valence + lexical_category * arousal + lexical_category * babiness

freq_only = ~ lexical_category * avg_freq

#full_surp_only = ~ lexical_category * avg_surprisal + lexical_category * MLU + lexical_category * final_frequency + lexical_category * solo_frequency + lexical_category * num_phons + lexical_category * concreteness + lexical_category * valence + lexical_category * arousal + lexical_category * babiness

full_surp_only = ~ lexical_category * avg_surprisal + lexical_category * num_phons + lexical_category * concreteness + lexical_category * valence + lexical_category * arousal + lexical_category * babiness


surp_only = ~ lexical_category * avg_surprisal

null_model = ~ 1

formulae <- formulas(~aoa, null_model, full_set, freq_only, full_surp, freq_surp, surp_only, full_surp_only)


```

Run cross validation for all models
```{r cv}

loo_data <- crossv_loo(ungroup(data))
  
fit_models <- function(id) {
  models <- "no model"
  #print("run model")
  train_idx <- loo_data[id,1][[1]][[1]]$idx
  test_idx <- loo_data[id,2][[1]][[1]]$idx
  train_data <- data[train_idx,]
  try(models <- fit_with(train_data, lm, formulae))
  
  result <- enframe(models) %>% 
    mutate(model = value,
      train = list(train_idx),
    test = list(test_idx)) %>% 
    select(-c(value))
  
  return(result)
  
}


models_loo<- loo_data$.id %>% map( ~ fit_models(.)) %>% reduce(rbind)
#Remove failed models

#mse_calc <- function(n){
#  test_data = data[loo_data$test[[n]],]
#  Y = test_data$aoa
#  Y_pred = predict(models_kfold$models[[n]],  test_data)
#  mse_ = mean((Y - Y_pred)^2)
#  return(as.numeric(mse_))
#}

#dev_calc <- function(id, model){
#  test_data = data[id,]
#  Y = test_data$aoa
#  Y_pred = predict(model,  test_data)
#  dev_ = abs(Y - Y_pred)
#  return(dev_)
#}

get_aoa_pred<- function(n){
   row <- tibble(
     name = models_loo$name[n],
     test = models_loo$test[n],
     train = models_loo$train[n],
     model = models_loo$model[n],
     test_word = data$words[as.numeric(test)],
     lexical_category = data$lexical_category[as.numeric(test)],
    aoa = data$aoa[as.numeric(test)],
    aoa_pred = predict(model[[1]],  data[as.numeric(test),]))
  return(row)
}

#models_kfold_try <- sep_models_kfold  %>% gather( key= model_name, value = "models", full_set, freq_only, full_surp, freq_surp, surp_only, full_surp_only)

sep_models_loo <- map(c(1:nrow(models_loo)), get_aoa_pred) %>% bind_rows() %>% 
  mutate(abs_dev = abs(aoa - aoa_pred)) %>% 
  mutate(se = abs_dev^2)


results <- sep_models_loo %>% 
  transform(abs_dev = as.numeric(abs_dev)) %>% 
  group_by(name) %>%
  summarise(mean_abs_dev = mean(abs_dev), rmse = sqrt(mean(se)), mse = mean(se))

  
```


Compare model performance by word
```{r byword}
test <- sep_models_loo %>% filter(name %in% c("full_set", "full_surp")) %>% 
  group_by(name, test_word, lexical_category) %>% summarise(mean(abs_dev)) %>% 
  spread(key=name, value="mean(abs_dev)" ) %>% 
  mutate(diff = full_set-full_surp) %>% 
  arrange(desc(diff))

test <- sep_models_loo %>% filter(name %in% c("freq_only", "surp_only")) %>% 
  group_by(name, test_word, lexical_category) %>% summarise(mean(abs_dev)) %>% 
  spread(key=name, value="mean(abs_dev)" ) %>% 
  mutate(diff = freq_only-surp_only) %>% 
  arrange(desc(diff))

plot_data = test
p = ggplot(data = plot_data %>% arrange(desc(diff)) %>% tail(50) , 
            aes(x = reorder(test_word,diff), y = diff, fill=lexical_category)) +
  geom_bar(stat='identity') +
  coord_flip()+
  scale_fill_discrete(name = "Lexical category", labels = c("nouns", "function words", "predicates"))+
  labs(x="", y="difference in absolute deviation") +
  theme(text=element_text(size=18,  family="Times New Roman"), legend.title = element_text( size = 16), legend.text = element_text( size = 16), legend.position = c(0.7, 0.6), axis.text.x = element_text(size = 16), axis.text.y = element_text(size = 16))

ggsave("megachild_absolutedeviation_diff_byword_top50.png",plot=p, width = 6, height = 10, units="in", limitsize = FALSE)

plot_data %>% group_by(lexical_category) %>% count()
plot_data %>% arrange(desc(diff)) %>% head(50) %>% group_by(lexical_category) %>% count()

plot_data %>% group_by(lexical_category) %>% summarise(mean=mean(diff))

```




View models trained on all data
```{r all_data_models}
all_data_models <- fit_with(data, lm, formulae)
plot(predict(all_data_models$full_surp), data$aoa)
summary(all_data_models$full_surp)
```


```{r cor_pred}

library(corrplot)
cor_data <- data %>% ungroup() %>% select(avg_surprisal, avg_freq, num_phons, concreteness, valence, arousal, babiness)
M <- cor(cor_data, method = "pearson")

#cor_data <- data %>% ungroup() %>% filter(lexical_category=="nouns") %>% select(!!predictors, aoa)
#cor(cor_data, method = "pearson")

png('corrplot_megachild.png')
corrplot(M, method="color",  
         type="upper", order="hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
         )
dev.off()
ggsave("corrplot_megachild.png",plot=p, width = 6, height = 5, units="in", limitsize = FALSE)

library(car)
model = lm(aoa ~ avg_surprisal + avg_freq + num_phons + concreteness + valence + arousal + babiness + lexical_category, data=data)
car::vif(model)

```



```{r betas}
get_betas <- function(n){
  model = full_surp_models$model[n]
  result <- tidy(model[[1]]) %>% 
      mutate(fold = n)
  return(result)
  }

full_surp_models= models_loo %>% filter(name=="full_surp")  

full_surp_betas = map(c(1:nrow(full_surp_models)), get_betas) %>% bind_rows()

full_surp_betas <- full_surp_betas %>% select(term, estimate, fold) %>% spread(key=term, value=estimate) %>% 
  mutate(noun_surprisal = avg_surprisal,
         fctwd_surprisal = avg_surprisal + lexical_categoryfunction_words + lexical_categoryfunction_words:avg_surprisal,
         pred_surprisal = avg_surprisal + lexical_categorypredicates + lexical_categorypredicates:avg_surprisal,
         noun_frequency = avg_freq,
         fctwd_frequency = avg_freq + lexical_categoryfunction_words + lexical_categoryfunction_words:avg_freq,
         pred_frequency = avg_freq + lexical_categorypredicates + lexical_categorypredicates:avg_freq
         ) %>% 
  select(noun_surprisal,fctwd_surprisal,pred_surprisal,noun_frequency, fctwd_frequency, pred_frequency) %>% 
  gather(key="term", value="estimate") %>% 
  separate(col=term, into=c("lexical_category", "term"), sep="_")



lex.labs <- c("function words", "nouns", "predicates")
names(lex.labs) <- c("fctwd", "noun", "pred")


p = ggplot(full_surp_betas, aes(x = estimate, y = term, colour = term)) +
  facet_grid(~ lexical_category, labeller = labeller(lexical_category = lex.labs)) +
  geom_vline(xintercept = 0, color = "grey", linetype = "dotted") +
  geom_point(size = 2, alpha = 0.2, position = position_jitter(w = 0, h = 0.3), show.legend = FALSE) +
  ggstance::stat_summaryh(geom = "point", size=4, shape=23, color="black", fill="greenyellow", fun.x = mean, fun.xmin = mean,fun.xmax = mean, show.legend = FALSE) +
  labs(x = "Coefficient estimate", y = "") +
  theme(text=element_text(size=18,  family="Times New Roman"), axis.text.x = element_text(size = 16), axis.text.y = element_text(size = 16))



ggsave("estimates_megachild.png",plot=p, width = 6, height = 4, units="in", limitsize = FALSE)

```


Get concreteness score for nouns/predicates/function words.

```{r concrete}
surp_model_data %>% unnest() %>% 
  group_by(lexical_category) %>% 
  select(concreteness, lexical_category) %>% 
  summarise(mean_score=mean(concreteness))

```